
## Getting Started

### Prerequisites

*   **Python 3.8+**
*   **Ollama:** Ensure Ollama is installed and running. You can download it from [https://ollama.com/](https://ollama.com/).
*   **Ollama Model:** Pull the desired model. The default is `qwen3:4b`.
    ```bash
    ollama pull qwen3:4b
    ```

### Installation

1.  **Clone the repository**

2.  **Create a virtual environment (optional):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

### Configuration

The main configuration is in <mcfile name="config.py" path="e:\rag_app\src\config.py"></mcfile>. You can modify the following:

*   `OLLAMA_BASE_URL`: The base URL for your Ollama instance (default: `"http://localhost:11434"`).
*   `OLLAMA_MODEL`: The Ollama model to use (default: `"qwen3:4b"`). Make sure this model is available in your Ollama setup.
*   `VECTORSTORE_PATH`: Path to save/load the FAISS vector store (default: `"vectorstore.faiss"`).
*   `UPLOAD_DIR`: Directory to temporarily store uploaded files (default: `"uploaded_files"`).

## How to Run

1.  Ensure Ollama is running and the specified model in `config.py` is downloaded.
2.  Navigate to the project's root directory in your terminal.
3.  Run the Streamlit application:
    ```bash
    streamlit run app.py
    ```
    This will open the application in your web browser.

## Usage

1.  **Upload Documents:**
    *   Use the file uploader to select one or more PDF, DOCX, or TXT files.
    *   The application will process these files, split them into chunks, and update/create the vector store. This might take some time depending on the size and number of documents.

2.  **Ask a Question:**
    *   Once the vector store is ready (either from new uploads or a previously existing one), the "Ask a Question" section will be available.
    *   Type your question related to the content of the uploaded documents into the text input field.
    *   Click the "Ask" button.

3.  **View Answer and Sources:**
    *   The application will display the answer generated by the LLM.
    *   You can expand the "Show Source Documents" section to see snippets from the original documents that were used as context for the answer.

4.  **Settings (Sidebar):**
    *   **Clear Uploaded Files:** Removes files from the `uploaded_files` directory and resets the file uploader.
    *   **Clear Vector Store:** Deletes the `vectorstore.faiss` file, effectively clearing the knowledge base. You will need to upload documents again to create a new one.
    *   The sidebar also displays the current Ollama model, URL, and vector store path.

## Key Technologies Used

*   **Streamlit:** For building the web application interface.
*   **Langchain:** For orchestrating the RAG pipeline (document loading, splitting, vector store interaction, QA chain).
*   **Ollama (via `langchain-community`):** For running local LLMs and generating embeddings.
*   **FAISS:** For creating and managing the vector store.
*   **PyPDF, python-docx, TextLoader:** For loading different document types.

